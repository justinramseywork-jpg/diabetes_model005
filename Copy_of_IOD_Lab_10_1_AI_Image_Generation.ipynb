{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justinramseywork-jpg/diabetes_model005/blob/main/Copy_of_IOD_Lab_10_1_AI_Image_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0dz9kTrA6hP"
      },
      "source": [
        "# Lab 10.1 - AI Image Generation using GPUs and Diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpSoTWEEA6hQ"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-0gF3f-A6hR"
      },
      "source": [
        "*Note*: This notebook is to be run in a Kaggle environment with a GPU:\n",
        "\n",
        "1. Go to https://www.kaggle.com/code after creating a free account at kaggle.com\n",
        "2. Go to File -> Import Notebook within the website and drag-and-drop this notebook file to upload it.\n",
        "3. Go to Settings -> Turn on Internet if it is not already enabled\n",
        "\n",
        "Later we will enable the GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZwbPIwrA6hR"
      },
      "source": [
        "The purpose of this lab is to learn how to develop generative image models using the diffusers library and GPU acceleration, enabling creative and controllable visual synthesis from text and images. The diffusers library by Hugging Face runs and trains diffusion models (https://huggingface.co/docs/diffusers/en/index).\n",
        "\n",
        "Diffusion models are trained to denoise random Gaussian noise step-by-step to generate a sample of interest, such as an image or audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BagDtNPkA6hR"
      },
      "source": [
        "## Initial Exploration\n",
        "\n",
        "Experiment with image prompting at sites such as https://deepai.org/, chatgpt/, http://gemini.google.com, https://x.com/i/grok\n",
        "\n",
        "Examples of prompts can be found at https://lexica.art/, krea.ai and https://www.midjourney.com/explore\n",
        "\n",
        "In this notebook images will be generated with different models, together with image-to-image transformations and interpolations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z68WZUhXA6hR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHz4uNLgA6hS"
      },
      "source": [
        "To enable the GPU go to Settings -> Accelerator -> GPU T4 x2. You have a limit of 30 hours of GPU use per month, so remember to turn off the GPU (Settings -> Accelerator -> None) when not needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_radVNZFarnP"
      },
      "outputs": [],
      "source": [
        "# After running this cell you should see\n",
        "# '/device:GPU:0' (it's ok if additional messages appear)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sf_PFILA6hS"
      },
      "source": [
        "The following cell installs the diffusers library (supported by the PyTorch 'torch' deep learning framework) and the transformers library. The transformers library (also by Hugging Face) makes it easy to use modern AI models without having to build them from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD7B2i4xa7DE"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers[\"torch\"] transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDP9nPNOA6hT"
      },
      "source": [
        "The `DiffusionPipeline` class allows one to generate images with a simple interface - all the model components are automatically loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYBaT8hKA6hT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzJE-0bgA6hT"
      },
      "outputs": [],
      "source": [
        "# Releases cached unused memory for reuse by PyTorch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Lx4qcMA6hT"
      },
      "source": [
        "## Unprompted image generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnCXENWlA6hT"
      },
      "source": [
        "To begin with, we generate images from a pretrained model that was trained on butterfly images. Unprompted images can be used to evaluate image quality, diversity, and realism without prompt bias. It helps visualise the range of possibilities that the model has learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLkRSXkJA6hT"
      },
      "outputs": [],
      "source": [
        "# If the kernel dies, simply rerun the previous two code cells and this cell\n",
        "# The .to(\"cuda\") method in PyTorch moves a model or tensor from the CPU to the GPU.\n",
        "generator = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\").to(\"cuda\")\n",
        "\n",
        "image = generator().images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UtLOrFxFB1H"
      },
      "source": [
        "Rerun the code below to see a new image is generated from a different random sample of noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWL_2aiEA6hT"
      },
      "outputs": [],
      "source": [
        "image = generator().images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_pujw8aA6hU"
      },
      "source": [
        "### Changing the number of inference steps\n",
        "\n",
        "Set num_inference_steps to 1, 2, 3 to see the image at early stages of generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM9SbbK6A6hU"
      },
      "outputs": [],
      "source": [
        "# 1 step\n",
        "image = generator(num_inference_steps=1).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0AFScv-A6hU"
      },
      "outputs": [],
      "source": [
        "# 2 steps\n",
        "# ANSWER:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXXZdOpyA6hU"
      },
      "outputs": [],
      "source": [
        "# 3 steps\n",
        "# ANSWER:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F87HN7w1A6hU"
      },
      "source": [
        "**Exercise**: Use the larger \"google/ddpm-celebahq-256\" model instead of \"anton-l/ddpm-butterflies-128\" with 100 inference steps to generate a realistic face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W8uLOvpA6hU"
      },
      "outputs": [],
      "source": [
        "# ANSWER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoePiP_yA6hU"
      },
      "source": [
        "## Prompted image generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tYV2_pDA6hU"
      },
      "source": [
        "Next we observe the impact of a detailed prompt using the popular Stable Diffusion model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhg4KjKWbU45"
      },
      "outputs": [],
      "source": [
        "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKgVNnRDb6Hm"
      },
      "outputs": [],
      "source": [
        "prompt = \"a panda playing guitar in a bamboo forest, cartoon style, soft lighting, in a playful mood\"\n",
        "# feel free to change this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn0pAGThcDVa"
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKFA9Jq9cHVS"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(\"cuda\").manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymi0qYLHcIu5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "image = pipeline(prompt, generator=generator).images[0]\n",
        "image #note that 50 is now the default number of inference steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DslidotA6hU"
      },
      "source": [
        "We can make this run faster on GPUs by setting torch_dtype to torch.float16 (the default is 32-bit precision)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-dQ99ihcLdZ"
      },
      "outputs": [],
      "source": [
        "%time\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True)\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
        "\n",
        "image = pipeline(prompt, generator=generator).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgSmpaRtdZxu"
      },
      "source": [
        "The following code enables a grid of sample images to be generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vAeY4lIdHV8"
      },
      "outputs": [],
      "source": [
        "def get_inputs(batch_size=1):\n",
        "    generator = [torch.Generator(\"cuda\").manual_seed(i) for i in range(batch_size)]\n",
        "    prompts = batch_size * [prompt]\n",
        "    num_inference_steps = 20\n",
        "\n",
        "    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7aqPPjQdbdb"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import make_image_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnJBxWuYde-T"
      },
      "outputs": [],
      "source": [
        "pipeline.enable_attention_slicing() #breaks large attention computations into smaller chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib53YYsldh4q"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "images = pipeline(**get_inputs(batch_size=8)).images\n",
        "make_image_grid(images, rows=2, cols=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL61DibFA6hV"
      },
      "source": [
        "We now try a more advanced model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z90FKGbA6hV"
      },
      "outputs": [],
      "source": [
        "pipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "pipe = DiffusionPipeline.from_pretrained(pipe_id, torch_dtype=torch.float16)\n",
        "pipe.to(\"cuda\");\n",
        "# it may take a couple of minutes for everything (~15GB) to be downloaded to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2Y8KJANA6hV"
      },
      "outputs": [],
      "source": [
        "#prompt = \"a cartoon of a 1980s car on a desert highway, midday sun\"\n",
        "\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjhNz51PA6hV"
      },
      "source": [
        "**Exercise**: Using the same pipe create two images - one with a less sophisticated and one with a more detailed prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKqQfym8A6hV"
      },
      "outputs": [],
      "source": [
        "# ANSWER\n",
        "easy_prompt =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vaAnZUXA6hW"
      },
      "outputs": [],
      "source": [
        "# ANSWER\n",
        "detailed_prompt =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJCXBdTA6hW"
      },
      "source": [
        "## Image to image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y6IldAEA6hW"
      },
      "source": [
        "In this section we take an existing image and edit it using prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnC9tvkrA6hW"
      },
      "outputs": [],
      "source": [
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import load_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz0UXDsJA6hW"
      },
      "outputs": [],
      "source": [
        "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
        "    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n",
        ")\n",
        "pipeline.enable_model_cpu_offload()\n",
        "# enables offloading of model weights from the GPU to the CPU when the GPU memory is insufficient\n",
        "# for the entire model.\n",
        "\n",
        "# ~13GB is downloaded here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXxXmrqhA6hW"
      },
      "outputs": [],
      "source": [
        "\n",
        "init_image = load_image('https://plus.unsplash.com/premium_photo-1674917000586-b7564f21540e').resize((512, 512))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIodxR7ZA6hW"
      },
      "outputs": [],
      "source": [
        "prompt = \"make this look like a painting\"\n",
        "image = pipeline(prompt, image=init_image, strength = 0.05, guidance_scale=1.0).images[0]\n",
        "# higher guidance scale - closer to prompt (default = 7.5)\n",
        "# higher strength - more creativity (default = 0.8)\n",
        "make_image_grid([init_image, image], rows=1, cols=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yGfEnZ4A6hW"
      },
      "source": [
        "**Exercise**: investigate what happens when the guidance scale and strength are changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtVr0vaGA6hW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucjx01dXA6hW"
      },
      "source": [
        "## Image interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USQIYSGFA6hW"
      },
      "source": [
        "Finally we see how we can interpolate images based on two distinct prompts. Feel free to change the prompts below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiI4HQDeA6hW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load Stable Diffusion pipeline\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "# Two text prompts\n",
        "prompt1 = \"a futuristic city at sunset\"\n",
        "prompt2 = \"a medieval village in winter\"\n",
        "\n",
        "# Get text embeddings\n",
        "text_input1 = pipe.tokenizer(prompt1, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "text_input2 = pipe.tokenizer(prompt2, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "embeds1 = pipe.text_encoder(text_input1)[0]\n",
        "embeds2 = pipe.text_encoder(text_input2)[0]\n",
        "\n",
        "# Interpolate between embeddings\n",
        "steps = 10\n",
        "interpolated_images = []\n",
        "\n",
        "for alpha in np.linspace(0, 1, steps):\n",
        "    blended_embed = (1 - alpha) * embeds1 + alpha * embeds2\n",
        "    image = pipe(prompt_embeds=blended_embed).images[0]\n",
        "    interpolated_images.append(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoYg2407A6hW"
      },
      "source": [
        "Observe the similarity of the prompts via cosine similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YiRZ1zbA6hW"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "similarity = cosine_similarity(embeds1, embeds2).mean()\n",
        "print(f\"Cosine similarity: {similarity.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ufui4YIA6hW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display results\n",
        "fig, axs = plt.subplots(1, steps, figsize=(20, 4))\n",
        "for i, img in enumerate(interpolated_images):\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdDSfGjsA6hW"
      },
      "source": [
        "We can save this as an image or video file for download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gd6WlhNA6hW"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "gif_path = \"interpolated_images.gif\"\n",
        "imageio.mimsave(gif_path, interpolated_images, fps=4)\n",
        "#download the image after finding the file under the /kaggle/working folder in the Output section in the sidebar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9gv5WPKA6hX"
      },
      "outputs": [],
      "source": [
        "imageio.mimsave(\"video.mp4\", interpolated_images, fps=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVUeBy1vA6hX"
      },
      "source": [
        "## BONUS\n",
        "\n",
        "Experiment with other models at https://huggingface.co/models using the image-related filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezh7aBd5A6hX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okvOeLnYA6hX"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We have learnt how to enable GPUs in Kaggle and use the diffusers library for image generation of diffusion models with different models and settings. We used unprompted and prompted image generation, then looked at the tasks of image-to-image changes via prompting as well as image interpolation.\n",
        "\n",
        "Turn off GPU usage by going to Settings -> Accelerator -> None in Kaggle. Under \"Session options\" on the right side panel you will be able to see how much of your 30-hour quota has been used up.\n",
        "\n",
        "Save and download this file for submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s9Ce0GnA6hX"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Zp1NLDA6hX"
      },
      "source": [
        "https://huggingface.co/docs/diffusers/en/index\n",
        "\n",
        "https://huggingface.co/docs/diffusers/main/using-diffusers/write_own_pipeline"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}